---
- name: Provision OCP HCP Cluster on AWS
  gather_facts: false
  hosts: localhost
# Required Variables for this playbook
#    aws_account_id: xxxxxxxxx
#    account_roles_prefix: Managed-OpenShift
#    operator_roles_prefix: Managed-OpenShift-HCP
#    cluster_name: xxxxxxx
#    kms_arn: arn:aws:kms:xxxxxxxx
#    vpc_cidr: 10.0.0.0/16
#    subnet_id1: subnet-xxxxxxxxxxxxx
#    subnet_id2: subnet-xxxxxxxxxxxxx
#    subnet_id3: subnet-xxxxxxxxxxxxx
#    compute_machine_type: m5.xlarge
  tasks:
    - set_fact:
        vpc_name: "{{ cluster_name }}-vpc"
        vpc_cidr_block: 10.0.0.0/16
        vpc_public_subnets:
          public-a:
            name: "{{ cluster_name }}-public-a"
            cidr: 10.0.0.0/20
            az: "{{ aws_region }}a"
          public-b:
            name: "{{ cluster_name }}-public-b"
            cidr: 10.0.16.0/20
            az: "{{ aws_region }}b"
          public-c:
            name: "{{ cluster_name }}-public-c"
            cidr: 10.0.32.0/20
            az: "{{ aws_region }}c"
        vpc_private_subnets:
          private-a:
            name: "{{ cluster_name }}-private-a"
            cidr: 10.0.128.0/20
            az: "{{ aws_region }}a"
          private-b:
            name: "{{ cluster_name }}-private-b"
            cidr: 10.0.144.0/20
            az: "{{ aws_region }}b"
          private-c:
            name: "{{ cluster_name }}-private-c"
            cidr: 10.0.160.0/20
            az: "{{ aws_region }}c"
        vpc_security_groups:
          - name: vpc
            description: "Allow internal traffic in the VPC"
            rules:
              - proto: all
                group_name: vpc
          - name: allow-public-ssh
            description: "Allow public SSH"
            rules:
              - proto: tcp
                cidr_ip: 0.0.0.0/0
                ports:
                  - 22
          - name: allow-public-http
            description: "Allow public web traffic"
            rules:
              - proto: tcp
                cidr_ip: 0.0.0.0/0
                ports:
                  - 80
                  - 8080
                  - 443

    - name: create VPC
      ec2_vpc_net:
        name: "{{ vpc_name }}"
        cidr_block: "{{ vpc_cidr_block }}"
        region: "{{ aws_region }}"
      register: create_vpc

    # parse the output of creating the VPC to extract the VPC ID -- we need to specify this in the subsequent tasks
    - name: "set fact: VPC ID"
      set_fact:
        vpc_id: "{{ create_vpc.vpc.id }}"

    # iterate over our dictionary of subnets with `with_dict`, and create each one with the Ansible module
    - name: create public VPC subnets
      ec2_vpc_subnet:
        vpc_id: "{{ vpc_id }}"
        cidr: "{{ item.value.cidr }}"
        az: "{{ item.value.az }}"
        tags:
          Name: "{{ item.value.name }}"
          kubernetes.io/role/elb: ''
      with_dict: "{{ vpc_public_subnets }}"
      register: create_public_vpc_subnets

    - name: create private VPC subnets
      ec2_vpc_subnet:
        vpc_id: "{{ vpc_id }}"
        cidr: "{{ item.value.cidr }}"
        az: "{{ item.value.az }}"
        tags:
          Name: "{{ item.value.name }}"
          kubernetes.io/role/internal-elb: ''
      with_dict: "{{ vpc_private_subnets }}"
      register: create_private_vpc_subnets

    # this is a tricky one, using some filters to:
    # - loop over the list of outputs from creating our subnets
    # - for that subnet, define a "name: id" entry in the `vpc_subnet_ids` dictionary (or empty dictionary if it doesn't exist)
    - name: "set fact: VPC subnet IDs from public"
      set_fact:
        vpc_subnet_ids: "{{ vpc_subnet_ids | default({}) | combine({ item.subnet.tags.Name: item.subnet.id }) }}"
      loop: "{{ create_public_vpc_subnets.results }}"

    - name: "set fact: add VPC subnet IDs from private"
      set_fact:
        vpc_subnet_ids: "{{ vpc_subnet_ids | combine({ item.subnet.tags.Name: item.subnet.id }) }}"
      loop: "{{ create_private_vpc_subnets.results }}"

    # iterate over our list of security groups and create each one with the Ansible module
    - name: create VPC security groups
      ec2_group:
        name: "{{ item.name }}"
        description: "{{ item.description }}"
        vpc_id: "{{ vpc_id }}"
        rules: "{{ item.rules }}"
      loop: "{{ vpc_security_groups }}"

    # create an EC2 key -- pretty simple
#    - name: create EC2 key
#      ec2_key:
#        name: "{{ vpc_key }}"

    # create a private hosted zone for the VPC with the Ansible module
#    - name: create Route 53 private hosted zone
#      route53_zone:
#        zone: "{{ vpc_dns_zone }}"
#        state: present
#        vpc_id: "{{ vpc_id }}"
#        vpc_region: "{{ aws_region }}"

    # create the internet gateway, saving the output to extract the ID later
    - name: create internet gateway
      ec2_vpc_igw:
        vpc_id: "{{ vpc_id }}"
      register: create_gateway

    # create the NAT gateway, looking up the subnet ID by the human readable name: "private-a"
    - name: create NAT gateway
      ec2_vpc_nat_gateway:
        subnet_id: "{{ vpc_subnet_ids[cluster_name + '-private-a'] }}"
        region: "{{ aws_region }}"
        tags:
          Name: "{{ cluster_name }}-nat"
        wait: yes
        if_exist_do_not_create: true
      register: create_nat_gateway

    # parse the outputs of the Ansible modules for some important details referred to when setting up routing
    - name: "set facts: Gateway IDs and IP"
      set_fact:
        vpc_gateway_id: "{{ create_gateway.gateway_id }}"
        vpc_nat_gateway_id: "{{ create_nat_gateway.nat_gateway_id }}"
        vpc_nat_gateway_ip: "{{ create_nat_gateway.nat_gateway_addresses[0].public_ip }}"

    # update the VPCs DNS with the public IP of the new NAT gateway
#    - name: update DNS with NAT gateway IP
#      route53:
#        zone: "{{ vpc_dns_zone }}"
#        private_zone: yes
#        record: nat.{{ vpc_dns_zone }}
#        type: A
#        value: "{{ vpc_nat_gateway_ip }}"

    # private route table that routes through the NAT -- attach it to our three private subnets
    - name: create route table for private subnets
      ec2_vpc_route_table:
        vpc_id: "{{ vpc_id }}"
        tags:
          Name: "{{ vpc_name }}-private"
        subnets:
          - "{{ vpc_subnet_ids[cluster_name + '-private-a'] }}"
          - "{{ vpc_subnet_ids[cluster_name + '-private-b'] }}"
          - "{{ vpc_subnet_ids[cluster_name + '-private-c'] }}"
        routes:
          - dest: 0.0.0.0/0
            gateway_id: "{{ vpc_nat_gateway_id }}"

    # public route table that routes through the internet gateway -- attach it to our three public subnets
    - name: create route table for public subnets
      ec2_vpc_route_table:
        vpc_id: "{{ vpc_id }}"
        tags:
          Name: "{{ vpc_name }}-public"
        subnets:
          - "{{ vpc_subnet_ids[cluster_name + '-public-a'] }}"
          - "{{ vpc_subnet_ids[cluster_name + '-public-b'] }}"
          - "{{ vpc_subnet_ids[cluster_name + '-public-c'] }}"
        routes:
          - dest: 0.0.0.0/0
            gateway_id: "{{ vpc_gateway_id }}"

    - name: Create ROSA Account-Wide IAM Roles
      command:
        argv:
          - rosa
          - create
          - account-roles
          - --hosted-cp
          - --prefix
          - Managed-OpenShift
          - --mode
          - auto
      register: account_roles

    - name: Get Returned Account Roles
      set_fact:
        arns: "{{account_roles.stdout}}"

    - name: Parse Worker, Installer, and Support Account Roles
      set_fact:
        worker_role: "{{ arns | regex_search('arn:aws\\S+Worker-Role') }}"
        installer_role: "{{ arns | regex_search('arn:aws\\S+Installer-Role') }}"
        support_role: "{{ arns | regex_search('arn:aws\\S+Support-Role') }}"

    - name: Create ROSA OIDC Configuration
      command:
        argv:
          - rosa
          - create
          - oidc-config
          - --mode
          - auto
          - --yes
      register: oidc_provider

    - name: Parse Returned OIDC Provider
      set_fact:
        oidc: "{{ oidc_provider.stdout | regex_search('[^/]+$') }}"

    - name: Get OIDC ID
      set_fact:
        oidc_id: "{{ oidc[:-1] }}"

    - name: Check if ROSA Operator Roles Exist
      command:
        argv:
          - rosa
          - list
          - operator-roles
          - --prefix
          - "{{ operator_roles_prefix | lower }}"
      register: existing_roles

    - name: Delete Existing ROSA Operator Roles
      command:
        argv:
          - rosa
          - delete
          - operator-roles
          - --prefix
          - "{{ operator_roles_prefix | lower }}"
          - -m
          - auto
          - -y
      when: existing_roles.stdout_lines | length > 1

    - name: Create ROSA Operator Roles
      command:
        argv:
          - rosa
          - create
          - operator-roles
          - --hosted-cp
          - --prefix
          - "{{ operator_roles_prefix }}"
          - --oidc-config-id
          - "{{ oidc_id }}"
          - --installer-role-arn
          - "arn:aws:iam::{{ aws_account_id }}:role/{{ account_roles_prefix }}-HCP-ROSA-Installer-Role"
          - --mode
          - auto
      register: operator_roles

    - name: Create ROSA HCP Cluster
      command:
        argv:
          - rosa
          - create
          - cluster
          - --cluster-name
          - "{{ cluster_name }}"
          - --hosted-cp
          - --mode
          - auto
          - --create-admin-user
          - --cluster-admin-password
          - "RedHat123RedHat123"
          - --operator-roles-prefix
          - "{{ operator_roles_prefix }}"
          - --oidc-config-id
          - "{{ oidc_id }}"
          - "--subnet-ids={{ vpc_subnet_ids[cluster_name + '-private-a'] }},{{ vpc_subnet_ids[cluster_name + '-private-b'] }},{{ vpc_subnet_ids[cluster_name + '-private-c'] }},{{ vpc_subnet_ids[cluster_name + '-public-a'] }},{{ vpc_subnet_ids[cluster_name + '-public-b'] }},{{ vpc_subnet_ids[cluster_name + '-public-c'] }}"
      register: cluster_create

    - debug:
        msg: "{{ cluster_create }}"

#      rosa create cluster \
#      --cluster-name amjed \
#      --domain-prefix amjed \
#      --sts \
#      --cluster-admin-password l1nuX$1855adMn \
#      --role-arn arn:aws:iam::338512279495:role/amjed3m-HCP-ROSA-Installer-Role \
#      --support-role-arn arn:aws:iam::338512279495:role/amjed3m-HCP-ROSA-Support-Role \
#      --worker-iam-role arn:aws:iam::338512279495:role/amjed3m-HCP-ROSA-Worker-Role \
#      --operator-roles-prefix amjed3m \
#      --oidc-config-id 2csnndgi4br20ssd2319347r9a0qu849 \
#      --region us-east-1 \
#      --version 4.14.33 \
#      --replicas 3 \
#      --compute-machine-type m5.xlarge \
#      --machine-cidr 10.0.0.0/16 \
#      --service-cidr 172.30.0.0/16 \
#      --pod-cidr 10.128.0.0/14 \
#      --host-prefix 23 \
#      --subnet-ids subnet-0d61e3c282236e63e,subnet-037c9f25a5173e9d7,subnet-0a06533fa0a1a9369,subnet-0f5cda30cad9137b4,subnet-019a1f23f0db6bdbe,subnet-0d76cb73631535f29 \
#      --disable-workload-monitoring \
#      --hosted-cp \
#      --billing-account 338512279495

  # Enter the following command to follow the OpenShift installer logs to track the progress of the installation

  #  rosa logs install --cluster=${CLUSTER_NAME}  --watch

  # When the installation is completed take a screenshot of the result of the command , the screenshot must cover the below message that indicate the result of the installation:
  # I: Cluster '${CLUSTER_NAME} ' has been successfully installed
  # Paste the screenshot at the bottom of this document.

  # Additional tasks as appropriate:
#  Install the appropriate SSL/TLS certificate files on the server.
#  Create specific application configurations.
#  Configure application logging based on "type".